% #############################################################################
% This is Chapter 3
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{State-of-the-Art on Microscopy Image Segmentation}
%\cleardoublepage
% The following line allows to ref this chapter
\label{chapter:state_of_the_art}

Fluorescence microscopy has recently become an important tool for the study of cells, as it allows the acquisition and visualization of \ac{3D} image volumes that extend deeper into the tissue. Therefore, as mentioned in Section \ref{section:motivation}, automated \ac{3D} microscopy image analysis techniques, specially to perform segmentation, are needed to efficiently and accurately quantify and characterize cells, nuclei, or other biological structures. In this section, we review the current state-of-the-art in microscopic image segmentation (with a particular focus on fluorescence microscopy).


\section{Microscopic image segmentation difficulties}

Segmentation of subcellular structures in microscopic images presents many challenges. In particular, during image acquisition, microscopic images often exhibit digital noise, background clutter, and blurring \cite{review:robust}. These problems are exacerbated for microscopic volumes because they are inherently anisotropic and anomalies vary along different axes. As a result, these images often exhibit the following: intensity inhomogeneity, low contrast, limited depth resolution, and consequently the subcellular structures being segmented typically have poorly defined edges \cite{active:inhmo}. 

When segmenting nuclei/cells, the challenge becomes even greater because the size, shape, and intracellular intensity heterogeneity of nuclei/cells vary widely and they are often grouped together in clumps so that they sometimes touch and/or overlap \cite{review:robust}. In recent decades, several automated segmentation methods for microscopic images have been investigated that aim to overcome some or all of these challenges.

%\subsection{Preprocessing Techniques}

\section{Classical approaches}
\label{subsection:clas}


In recent years, several classical image processing algorithms have been investigated. In particular, \ac{ACMs} have been widely used in microscopy image segmentation (with a particular focus on fluorescence microscopy) due to their ability to segment structures with different shapes. \ac{ACMs} iteratively minimize an energy/cost function while deforming an initial contour to fit objects of interest. There are several variants of active contours. One is edge-based \ac{ACM}, which uses image gradient maps in object identification \cite{snakes:active}. However, these approaches are sensitive to image noise and depend heavily on the placement of the initial contour. Active contours have also been integrated with region-based approaches, which aim to find an energy balance between foreground and background regions \cite{region:based}. Region-based methods generally achieve better results than edge-based active contours because they are relatively independent of initial contour generation and robust to noise. 


In \cite{3D:active}, a \ac{3D} active surface method is proposed as an extension of the region-based \ac{2D} active contour model of Chan-Vese presented in \cite{region:based} to segment \ac{3D} cell structures of a rat kidney. In \cite{active:inhmo}, a method for segmenting cell nuclei in \ac{3D} microscopy volumes based on a combination of \ac{3D} region-based active contours and \ac{3D} inhomogeneity correction was described. A dataset containing \ac{3D} volumes of cell structures from a rat kidney was used. This method achieved an average accuracy of 89.58\%, outperforming the previous method \cite{region:based}. Another well-known biomedical imaging tool is Squassh \cite{squass:original,squassh}, which minimizes an energy function derived from a generalized linear model to segment and quantify \ac{2D} or \ac{3D} subcellular structures. The main problem with these methods is that they are unable to separate overlapping nuclei, which degrades the quality of the segmentation results. 

To separate overlapping cells/nuclei and improve segmentation, several methods have been described. In \cite{active:couple}, a model based on coupled active surfaces for cell segmentation is presented. This framework uses multiple active surfaces coupled by a penalty for overlap and a volume conservation constraint that improves the contouring of cell boundaries \cite{couple:original}. \citet{active:couple} improved this technique, first presented in \cite{couple:original}, by incorporating watershed techniques, a \ac{non-PDE}-based energy minimization, and the Radon transform to improve the separation of touching cells. The evaluation of this method was performed in \ac{3D} time-lapse fluorescence microscopy image datasets of HeLa cells and achieved an average precision of 98.98\%. Moreover, this approach proved to be computationally much more efficient (up to nine times faster) compared to \cite{couple:original}. In \cite{graphs}, Arslan et al. proposed an alternative, a new model-based nuclei segmentation algorithm that defines primitives to represent the boundaries of the nucleus and region growing to delineate the nucleus borders. The model was evaluated on two challenging datasets of fluorescence microscopy images of human hepatocellular carcinoma cell lines and achieved an average precision of 72\%. The experiments in this work show that it leads to better results on overlayed nuclei and is less susceptible to noise.

However, these methods require manual optimization of parameters, are difficult to generalize to different datasets, and are unable to discriminate between different cellular structures. 
 
\section{Convolutional Neural Networks}

Recently, deep learning-based models have gained recognition due to their ability to automatically learn important features from data. In the last decades they have been successfully applied to computer vision tasks, including microscopy image analysis, for nuclei detection, cell segmentation, tissue segmentation, image classification, and so on. Many of them were able to outperform the classical approaches mention in Section \ref{subsection:clas} \cite{review_cnn}.

\subsection{Supervised models}

One popular deep architecture is the \ac{CNN} which, given images and corresponding annotations (ground-truth), is able to learn the essential features of an image that are invariant to irrelevant variations \cite{guide:cnn}. \ac{CNNs} have already been successfully implemented in various computer vision tasks, including microscopy segmentation.

In \cite{CNN2}, Xing et al. implemented an automatic nuclei segmentation method using a \ac{CNN}, for nuclei detection, together with a selection-based sparse shape model. The proposed method was able to achieve superior performance, when compared with other classical models, across three different datasets. In \cite{CNN3}, a \ac{CNN} is used to produce a ternary map, contrary to the binary map proposed in \cite{CNN2}. The additional third class is used to identify pixels on the nuclear boundaries, which helps in the segmentation of crowded and sparse nuclei. Another contribution of this paper include the release of a new dataset, MultiOrgan dataset, with a diversity of nuclear appearances from seven organs and a new evaluation metric to better measure the performance of a nuclear segmentation method called \ac{AJI}. The approach obtained reasonable results for different datasets, which shows its generatization hability, and an overall \ac{AJI} of 0.508 and F1-Score of  0.827, outperforming \cite{CNN2} and other open source softwares \cite{cellprofiler}. 

Though these techniques produce good results in \ac{2D} images, they're not applicable for segmentation of \ac{3D} microscopic image volumes, because they cannot utilize the depth information in a volume. In \cite{Unet:3D}, Çiçek et al. successfully segmented volumetric microscopic images of the Xenopus kidney with a \ac{3D} U-net, by expanding the previous \ac{2D} U-net \cite{Unet:2D} architecture. A more detailed description of the model was presented in subsection \ref{subsection:3dunet}. The metric used to evaluate the model was \ac{IoU}, and the results for the \ac{3D} U-Net yielded an average value of 0.704 compared to the \ac{2D} U-Net which yielded an average \ac{IoU} value of 0.547. However, the results of the experiments also show that the performance of the network is highly dependent on the amount of annotated data available, as it performed better in semi-automatic segmentation than in fully automatic segmentation.

% https://towardsdatascience.com/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1 - caso queiras completar

Supervised deep neural network methods, like the ones mentioned above, require a large amount of pixel-wise annotated data for training. Obtaining these detailed annotations is not only time consuming, especially for \ac{3D} volumes, but also must be done by an expert. Therefore, the interest in weakly supervised/fully unsupervised deep learning models that perform well on data without annotation has increased significantly in recent years.

\subsection{Weakly Supervised / Unsupervised models}

In \cite{weakly:2D} and \cite{weakly:3D}, weakly supervised methods were successfully used to segment cell nuclei in \ac{2D} and \ac{3D} microscopy images, respectively. \citet{weakly:2D} proposed the use of points annotation for nuclei segmentation. From these annotations, two types of coarse labels are derived using the Voronoi diagram and the k-means clustering algorithm. These labels are then used to train a \ac{CNN} with cross-entropy loss. The authors also implement a dense CRF loss to refine the trained model. The performance of the model was evaluated using the MultiOrgan dataset of \cite{CNN3}, for which an accuracy of 0.9097 and an \ac{AJI} value of 0.5174 were obtained. It has been shown that the performance of the weakly supervised method is close to the fully supervised models with the same network structure and, moreover, the annotation time spent on each image is greatly reduced. However, from the \ac{AJI} value, we can also conclude that the nuclear shapes and separation can still be improved.

In \cite{weakly:3D}, a model for segmenting \ac{3D} instances is proposed that uses weak annotations for training. Detection of all instances of interest is achieved using \ac{3D} bounding boxes, and segmentation is achieved for all detected instances by using the full voxel annotation only for a small subset of the instances. For segmentation, the authors use a Fully Convolutional Network backbone with VoxRes block \cite{voxresnet}. The performance of the detection model is compared with the supervised method VoxResNet \cite{voxresnet} and it was found to achieve similar performance with less annotation time. However, the annotation time is still very long (at least 5.5 hours).

Although these methods help to minimize the amount of work required to label images, approaches have been presented in the literature that allow for completely unsupervised segmentation of medical images. \citet{SOTA:3DCNN} proposed an unsupervised approach to segment cell nuclei from \ac{3D} fluorescence microscopy images. For this purpose, a \ac{3D} \ac{CNN} with an encoder-decoder struture was constructed and trained with generated synthetic microscopy volumes and synthetic ground truth volumes containing multiple cell nuclei. The performance of the model was compared with the previously discussed active surface models \cite{3D:active,active:inhmo}, Squassh \cite{squassh}, and a \ac{2D} \ac{CNN} model \cite{2dplus}, and it achieved better results than the first two and similar results to the \ac{2D} \ac{CNN}, with an average accuracy of 92.93\%. However, the false detection rate is higher than the best results, i.e., the model tends to over-segment. In \cite{3d:detection} the authors present a method, based on \cite{SOTA:3DCNN}, for detection and segmentation of cell nuclei in \ac{3D} fluorescence microscopy images. In this approach, each nuclear center is detected using adaptive \ac{3D} histogram equalization, \ac{3D} distance transformation, and \ac{3D} classification \ac{CNN}. Then, the nuclei surrounding the seeds are segmented using a \ac{3D} segmentation \ac{CNN}. This approach has been shown to perform better than \cite{SOTA:3DCNN}, but is unable to detect all the nuclei, resulting in under-segmentation (high false negative rate). 

\section{Generative Adversarial Networks}

The performance of \ac{CNN} architectures is always limited by the quantity and quality of the dataset used for training. This is where the \ac{GAN} model comes into play to significantly improve the performance of these approaches due to their superior ability to generate data as well as translate it, as mentioned in Section \ref{section:GANs}.

\subsection{Supervised models}

In \cite{cCGAN}, a robust transfer learning framework for the segmentation of HEp-2 Specimen image segmentation using \ac{GAN} is presented. A novel conditional generative adversarial network with classifier (cC- GAN) is introduced to solve the overfitting problem of most DL models by improving their transfer capacity. Using this method, a segmentation accuracy of 75.27\% was achieved in the MIVIA dataset, showing the great potential of applying \ac{GAN} models to microscopic segmentation problems.

\subsection{Weakly Supervised / Unsupervised models}

In \cite{weakly:GAN}, a weakly supervised approach to nucleus segmentation is proposed. A point labelling is used as a weak labelling, then the author uses a Pix2Pix network model to detect the centroid of the nucleus and build a likelihood map, a guided propagation to build a pixel contribution map of the nucleus, and a graph cut to obtain the final instance segmentation. The proposed approach is able to outperform the fully supervised U-Net \cite{Unet:2D} model.

In a multi-organ study for nuclei segmentation on histopathology images \cite{cGAN:cycleGAN}, the authors used CycleGAN and \ac{cGAN} models. First, a CycleGAN model was trained with images from four different organs to synthetically generate pathology data along with perfectly segmented nuclei labels. The real and synthetically generated images were then used to train a \ac{cGAN} to perform nuclei segmentation. This segmentation network showed a 29.19\% improvement in \ac{AJI} compared to the previously mentioned supervised model \ac{CNN}-3C \cite{CNN3} and of 73.19\% compared to the U-Net model \cite{Unet:2D}.


The work proposed in \cite{SOTA:3DCNN} was improved in \cite{3D:CycleGAN} by integrating \ac{GAN}. A challenging problem in previous work has been the difficulty in generating realistic synthetic microscopic data to train the \ac{CNN}, due to the natural diversity of biological structures. \citet{3D:CycleGAN} addressed this problem with a modified model of CycleGAN, a spatially constrained CycleGAN, SpCycleGAN. The spatially constrained CycleGAN is used to generate \ac{3D} realistic synthetic training data. Then, a modified \ac{3D} U-Net network is trained with these \ac{3D} synthetic data to segment nuclei structures. The final average accuracy result was 94.59\%, outperforming \cite{SOTA:3DCNN}. However, because this model has some difficulty separating cell nuclei, an extension of this approach for nuclei detection and segmentation was presented in \cite{detection:3D}, using the synthetically generated images from SpCycleGAN to train a \ac{CNN} architecture for classification and segmentation.

DeepSynth \cite{deepsynth} is currently recognized as the state-of-the-art deep model for unsupervised \ac{3D} nuclei segmentation, and is based on the previously referenced work \cite{2dplus,SOTA:3DCNN,3D:CycleGAN}.

More recently, in 2021, \cite{adgan} treats the segmentation of cell nuclei as an image-to-image translation problem. \citet{adgan} propose a novel end-to-end unsupervised framework called \ac{AD-GAN}. They performed segmentation of cell nuclei in challenging \ac{2D} and \ac{3D} datasets of microscopic images. This work addresses the problem of lossy transformation \cite{lossy:cyclegan}, which is defined by the content inconsistency between the original images and the corresponding segmentation masks obtained by the model. These inconsistencies include: the deletion/addition of nuclei at the macro level, shape differences at the micro level, and a location offset. With this new and unsupervised model of \ac{GAN}, the authors were able to mitigate the problem of lossy transformations at the macro and micro levels and even extend this work to instance segmentation. The results obtained for the \ac{3D} fluorescence image dataset Scaffold \cite{dataset} were a \ac{DC} of 83.3\%. This is significantly higher than the compared models, the DeepSynth model \cite{deepsynth} (45.8\%) and other well-known biomedical imaging tools such as Cell-Profiler 3.0 \cite{cellprofiler} (53.1\%) and Squassh (62.3\%) \cite{squassh}. 

In summary, several approaches have been proposed for the segmentation of subcellular structures in microscopic images, and some of the best results have been obtained with deep learning-based models. These models can be trained in a supervised or weakly/unsupervised manner. The advantage of unsupervised models is their independence from annotated data for training. State-of-the-art unsupervised approach presented in \cite{deepsynth} used the CycleGAN as a two-stage pipeline to train a robust segmentor with CycleGAN-synthesized data. However, this adds time and computational costs and significantly increases the complexity of the system. In this work, a different approach is proposed, more similar to the \ac{AD-GAN} model (but without making major changes to the CycleGAN architecture), which consists in using an unsupervised end-to-end CycleGAN model to segment subcellular strutures, nuclei and Golgi, in fluorescence microscopy images. 

Finally, the different works that were cited in this brief review are summarized in table \ref{tab:state-of-the-art-table}.

% Please add the following required packages to your document preamble:
% \usepackage{lscape}
% \usepackage{longtable}
% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
\begin{landscape}
    \begin{longtable}{c|l|l|l|c|l|}
    \caption{Deep Learning state-of-the-art for microscopic image segmentation.}
    \label{tab:state-of-the-art-table}\\
    \cline{2-6}
                                                                & \multicolumn{1}{c|}{Main Goal}                                                                                                                                   & \multicolumn{1}{c|}{Dataset}                                                                                                                                                                     & \multicolumn{1}{c|}{Model}                                                                                                                                                   & \begin{tabular}[c]{@{}c@{}}Supervised \\ or Unsupervised\end{tabular} & \multicolumn{1}{c|}{Main Results}                                                                                                                                                                                                                                                                                   \\ \hline
    \endfirsthead
    %
    \multicolumn{6}{c}%
    {{\bfseries Table \thetable\ continued from previous page}} \\
    \cline{2-6}
                                                                & \multicolumn{1}{c|}{Main Goal}                                                                                                                                   & \multicolumn{1}{c|}{Dataset}                                                                                                                                                                     & \multicolumn{1}{c|}{Model}                                                                                                                                                   & \begin{tabular}[c]{@{}c@{}}Supervised \\ or Unsupervised\end{tabular} & \multicolumn{1}{c|}{Main Results}                                                                                                                                                                                                                                                                                   \\ \hline
    \endhead
    %
    \multicolumn{1}{|c|}{\cite{3D:active}}     & \begin{tabular}[c]{@{}l@{}}Segmentation of \\ cellular objects \\ in fluorescence \\ microscopy images\end{tabular}                                         & \begin{tabular}[c]{@{}l@{}}\ac{3D} cellular \\ structures of a rat \\ kidney\end{tabular}                                                                                       & \begin{tabular}[c]{@{}l@{}}Modified active contour (active\\ surface) model\end{tabular}                                                                                     & Unsupervised                                                        & \multicolumn{1}{c|}{-}                                                                                                                                                                                                                                                                                              \\ \hline
    \multicolumn{1}{|c|}{\cite{active:inhmo}}  & \begin{tabular}[c]{@{}l@{}}Segmentation\\ of nuclei in \ac{3D} \\ microscopy \\ volumes\end{tabular}                                         & \begin{tabular}[c]{@{}l@{}}\ac{3D} volumes of \\ a rat kidney \\ labeled with \\ Hoechst 33342\end{tabular}                                                                     & \begin{tabular}[c]{@{}l@{}}Combination of \ac{3D} region-\\ based active contours \\ and \ac{3D} inhomogeneity \\ correction\end{tabular}                                              & Unsupervised                                                        & \begin{tabular}[c]{@{}l@{}}Accu: 91.87\% (DS-II), \\ 89.65\% (DS-III), 87.71\% (DS-IV), \\ 89.10\% (DS-V)\end{tabular}                                                                                                                                                                                               \\ \hline
    \multicolumn{1}{|c|}{\cite{squassh}}       & \begin{tabular}[c]{@{}l@{}}Detection, delineament,\\ and quantification \\ of sub-cellular \\ structures in \\ fluorescence \\ microscopy images\end{tabular} & \multicolumn{1}{c|}{-}                                                                                                                                                                           & \begin{tabular}[c]{@{}l@{}}Squassh (Segmentation and \\ QUAntification of \\ Subcellular SHapes)\end{tabular}                                                                & Unsupervised                                                        & \multicolumn{1}{c|}{-}                                                                                                                                                                                                                                                                                              \\ \hline
    \multicolumn{1}{|c|}{\cite{active:couple}} & \begin{tabular}[c]{@{}l@{}}Multi-cell segmentation \\ and tracking \\ in Time-Lapse \\ Fluorescence \\ Microscopy\end{tabular}                                & \begin{tabular}[c]{@{}l@{}}Four raw \ac{3D} time-lapse \\ fluorescence microscopy \\ image datasets of HeLa cells \\ (Hoechst, H2B-GFP, \\ RAD18-YFP, PCNA-GFP)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Coupled-active-surfaces \\ framework with watershed \\ techniques, a \ac{non-PDE}-based \\ energy minimization and \\ the Radon transform\end{tabular} & Unsupervised                                                        & \begin{tabular}[c]{@{}l@{}}Precision: 99.7\% (Hoechst), \\ 100\% (H2B-GFP), \\ 97.1\% (RAD18-YFP), \\ 99.1\% (PCNA-GFP)\end{tabular}                                                                                                                                                                                   \\ \hline
    \multicolumn{1}{|c|}{\cite{graphs}}        & \begin{tabular}[c]{@{}l@{}}Segmentation of\\ nuclei in fluorescence \\ microscopy images\end{tabular}                                                     & \begin{tabular}[c]{@{}l@{}}Two datasets of fluorescence \\ microscopy images of human \\ hepatocellular carcinoma \\ (Huh7 and HepG2) cell lines\end{tabular}                                    & \begin{tabular}[c]{@{}l@{}}Definition of primitives to \\ construct an attributed \\ Relational Graphs\end{tabular}                                                          & Unsupervised                                                        & \begin{tabular}[c]{@{}l@{}}Precision: 78.28\% (Huh7), \\ 65.75\% (HepG2); \\ Recall: 88.44\% (Huh7), \\ 85.51\% (HepG2);\end{tabular}                                                                                                                                                                               \\ \hline
    \multicolumn{1}{|c|}{\cite{CNN2}}          & \begin{tabular}[c]{@{}l@{}}Automatic nucleus\\ segmentation applicable\\ to histopathology \\ images with different \\ stains\end{tabular}                  & \begin{tabular}[c]{@{}l@{}}Histopathology images:\\ pancreatic neuroendocrine \\ tumor, brain tumor \\ and breast cancer\end{tabular}                                                            & Two-Class \ac{CNN}                                                                                                                                          & Supervised                                                            & \begin{tabular}[c]{@{}l@{}}Mean Dice similarity \\ coefficient: 0.85 (brain), \\ 0.92 (NET), 0.80 (breast)\end{tabular}                                                                                                                                                                                             \\ \hline
    \multicolumn{1}{|c|}{\cite{CNN3}}          & \begin{tabular}[c]{@{}l@{}}Nuclei segmentation \\ at instance level \\ and release a dataset \\ of labeled images\end{tabular}                                  & \begin{tabular}[c]{@{}l@{}}Hematoxylin and \\ Eosin (H\&E)-stained \\ tissue images\end{tabular}                                                                                              & Three-Class \ac{CNN}                                                                                                                                        & Supervised                                                            & \begin{tabular}[c]{@{}l@{}}AJI: 0.5083\\ Average Haudorff distance: 7.6615\\ Average Dice's Coefficient: 0.7623\\ F1-Score: 0.8267\end{tabular}                                                                                                                                                                     \\ \hline
    \multicolumn{1}{|c|}{\cite{Unet:3D}}       & \begin{tabular}[c]{@{}l@{}}Dense volumetric \\ Segmentation\end{tabular}                                                                                         & \begin{tabular}[c]{@{}l@{}}Miscroscopic dataset\\  of the Xenopus kidney\end{tabular}                                                                                                            & \ac{3D} U-Net                                                                                                                                                                     & Supervised                                                            & \begin{tabular}[c]{@{}l@{}}Semi-automated segmentation - \\ Average IoU: 0.863\\ Fully-automated segmentation - \\ Average IoU: 0.704\end{tabular}                                                                                                                                                                  \\ \hline
    \multicolumn{1}{|c|}{\cite{weakly:2D}}     & \begin{tabular}[c]{@{}l@{}}Nuclei segmentation \\ in histopathology \\ images\end{tabular}                                                                       & \begin{tabular}[c]{@{}l@{}}Lung Cancer dataset and \\ MultiOrgan dataset \\ (from \cite{CNN3})\end{tabular}                                                                     & \begin{tabular}[c]{@{}l@{}}Derive two types of coarse labels \\ from the points annotation \\ and use them to train a \ac{CNN}\end{tabular}                 & Weakly supervised                                                     & \begin{tabular}[c]{@{}l@{}}Lung Cancer (ground-truth points):\\ Pixel-level: Accu: 0.9427 \\ and F1: 0.8143; \\ Object-level: Dice: 0.8021 \\ and \ac{AJI}: 0.6497 \\ MultiOrgan (ground-truth points): \\ Pixel-level: Accu: 0.9097 \\ and F1: 0.7716; \\ Object-level: Dice: 0.7242 \\ and \ac{AJI}: 0.5174\end{tabular} \\ \hline
    \multicolumn{1}{|c|}{\cite{weakly:3D}}     & \begin{tabular}[c]{@{}l@{}}Instance segmentation \\ of  nuclei in \ac{3D} \\ Biomedical Images\end{tabular}                                     & \begin{tabular}[c]{@{}l@{}}Nuclei of HL60 cells, \\ microglia cells (in-house), \\ and C.elegans developing \\ embryos\end{tabular}                                                              & \begin{tabular}[c]{@{}l@{}}\ac{3D} bounding box annotation and\\ \ac{FCN} backbone with \\ VoxRes block for segmentation\end{tabular}                            & Weakly supervised                                                     & \begin{tabular}[c]{@{}l@{}}Segmentation: F1: \\ 0.8927 (HL60 cells);  \\ 0.8424 (microglia cells); \\ 0.9495 (C.elegans);\\ Annotation time: 5.5h (HL60 cells); \\ 54.7h (microglia cells)\end{tabular}                                                                                                           \\ \hline
    \multicolumn{1}{|c|}{\cite{SOTA:3DCNN}}    & \begin{tabular}[c]{@{}l@{}}Nuclei segmentation \\ of fluorescence \\ miscroscopy \\ for \ac{3D} images\end{tabular}                             & \begin{tabular}[c]{@{}l@{}}Rat kidney datasets. \\ Greyscale images of size \\ X=512 x Y=512. \\ Data-I: Z=512 images.\end{tabular}                                                              & \begin{tabular}[c]{@{}l@{}}\ac{3D} \ac{CNN}  (trained with \\ synthetic volumes\\ automatically generated)\end{tabular}                                                                 & Unsupervised                                                          & \begin{tabular}[c]{@{}l@{}}Accuracy for three subvolumes of\\ Data-I: 92.20\%; 92.32\%; 94.26\%\\ Type-I error (false detection rate): \\ 5.38\%; 6.81\%; 5.19\%\\ Type-II error (missed detection rate): \\ 2.42\%; 0.87\%; 0.55\%\end{tabular}                                                                    \\ \hline
    \multicolumn{1}{|c|}{\cite{3d:detection}}  & \begin{tabular}[c]{@{}l@{}}Detection and \\ segmentation of \\ nuclei in fluorescence \\ microscopy images\end{tabular}                                       & Rat kidney datasets.                                                                                                                                                                            & \begin{tabular}[c]{@{}l@{}}Detection of nucleus using a \ac{3D} \\ distance transform and \\ segmentation with \ac{3D} \\ classification \ac{CNN}\end{tabular}                              & Unsupervised                                                          & \begin{tabular}[c]{@{}l@{}}Accuracy for three subvolumes of\\ Data-I: 93.30\%; 93.48\%; 94.67\%\\ Type-I error (false detection rate): \\ 1.65\%; 1.74\%; 1.56\%\\ Type-II error (missed detection rate): \\ 5.05\%; 4.78\%; 3.77\%\end{tabular}                                                                    \\ \hline
    \multicolumn{1}{|c|}{\cite{cCGAN}}         & \begin{tabular}[c]{@{}l@{}}Segmentation of \\ different Human \\ Epithelial type 2 \\ (HEp-2) cell images\end{tabular}                                           & I3A and MIVIA                                                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}Conditional generative adversarial \\ networks with classifier (cC-GAN)\end{tabular}                                                              & Supervised                                                            & Accuracy: 75.27\% (MIVIA)                                                                                                                                                                                                                                                                                           \\ \hline
    \multicolumn{1}{|c|}{\cite{weakly:GAN}}    & \begin{tabular}[c]{@{}l@{}}Segmentation of \\ nuclei pathological \\  images\end{tabular}                                                                        & \begin{tabular}[c]{@{}l@{}}Multi-Organ dataset \\ from \cite{CNN3}\end{tabular}                                                                                                 & \begin{tabular}[c]{@{}l@{}}Point labelling used as anotattion. \\ Pix2pix network model for nucleus \\ detection and graph-cut \\ to instance segmentation\end{tabular}      & Weakly supervised                                                     & F-measure: 0.913; mDice: 0.664                                                                                                                                                                                                                                                                                      \\ \hline
    \multicolumn{1}{|c|}{\cite{cGAN:cycleGAN}} & \begin{tabular}[c]{@{}l@{}}Multi-Organ Nuclei \\ segmentation in\\  Histopathology \\ Images\end{tabular}                                                        & \begin{tabular}[c]{@{}l@{}}Multi-Organ dataset \\ from \cite{CNN3}\end{tabular}                                                                                                 & \begin{tabular}[c]{@{}l@{}}Conditional GAN (\ac{cGAN}) \\ network trained on synthetic\\ data generated by CycleGAN\end{tabular}                                                  & Unsupervised                                                          & \begin{tabular}[c]{@{}l@{}}AJI: 0.721; \\ Average Hausdorff Distance: 4.291; \\ F1-Score: 0.866\end{tabular}                                                                                                                                                                                                                                  \\ \hline
    \multicolumn{1}{|c|}{\cite{3D:CycleGAN}}   & \begin{tabular}[c]{@{}l@{}}Nuclei segmentation \\ of fluorescence \\ miscroscopy \\ for \ac{3D} images\end{tabular}                             & Rat kidney datasets.                                                                                                                                                                            & \begin{tabular}[c]{@{}l@{}}Modified \ac{3D} U-Net trained with \\ three dimensional synthetic data \\ generated using spatially \\ constrained CycleGAN\end{tabular}             & Unsupervised                                                          & \begin{tabular}[c]{@{}l@{}}Accuracy for three subvolumes of\\ Data-I: 95.56\%; 93.67\%; 94.54\%\\ Type-I error (false detection rate): \\ 2.57\%; 5.65\%; 5.10\%\\ Type-II error (missed detection rate): \\ 1.86\%; 0.68\%; 0.36\%\end{tabular}                                                                    \\ \hline
    \multicolumn{1}{|c|}{\cite{detection:3D}}  & \begin{tabular}[c]{@{}l@{}}Detection and \\ segmentation of \\ nuclei in fluorescence \\ microscopy images\end{tabular}                                       & Rat kidney datasets.                                                                                                                                                                            & \begin{tabular}[c]{@{}l@{}}Two \ac{CNNs} models to detect and \\ segment nuclei trained on \\ volumes generated by the\\ SpCycleGAN\end{tabular}                                  & Unsupervised                                                          & \begin{tabular}[c]{@{}l@{}}Precision: 93.47\%; Recall: 96.80\% ;\\  F1 Score: 95.10\%\end{tabular}                                                                                                                                                                                                                  \\ \hline
    \multicolumn{1}{|c|}{\cite{adgan}}    & \begin{tabular}[c]{@{}l@{}}Cell nuclei \\ segmentation \\ in \ac{2D} and \ac{3D} \\ images\end{tabular}                                                                    & \begin{tabular}[c]{@{}l@{}}Fluo-N2DL-HeLa (\ac{2D}); \\ HaCaT (\ac{2D}); \\ BBBC024 (\ac{3D}); \\ Scaffold-A549 (\ac{3D})\end{tabular}                                                                               & \begin{tabular}[c]{@{}l@{}}Aligned Disentangling\\  Generative Adversarial \\ Network (AD-GAN)\end{tabular}                                                                  & Unsupervised                                                          & \begin{tabular}[c]{@{}l@{}}Precision: 93.8 (BBBC024); \\ 89.0 (Scaffold-A549)\\ DICE: 92.6 (BBBC024); \\ 83.3 (Scaffold-A549)\\ Recall: 91.5 (BBBC024); \\ 78.2 (Scaffold-A549)\end{tabular}                                                                                                                        \\ \hline
    \end{longtable}
    \end{landscape}