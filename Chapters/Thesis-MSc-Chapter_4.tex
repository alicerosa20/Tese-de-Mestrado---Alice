% #############################################################################
% This is Chapter 4
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Methodology}
\cleardoublepage
% The following line allows to ref this chapter
\label{chapter:methodology}

% Deixar para o final, mas inserir aqui uma pequena introdução do que vais falar neste capitulo

As mentioned earlier, the goal of this work is to segment fluorescent microscopic images using Deep Learning methods. The first phase is to implement supervised models to accomplish this task of segmenting the nuclei and Golgi of these images. Then we focus on the main goal, which is to implement an unsupervised model that can perform the same task as well or better than the supervised models. And finally, we add a third class to detect the nucleus-Golgi pairs.

This chapter begins by presenting the dataset used to train, validate, and test these models. The proposed approach to semantic segmentation using an unsupervised model is described in section \ref{section:proposed}. The performance of this approach is compared to the supervised models, which are described in detail in section \ref{section:comparison}. Then, section \ref{section:models_inference} describes how the models are used for inference after training. Finally, section \ref{section:metrics} describes the metrics used to measure the performance of these models.

%As mentioned earlier, the goal of this work is to segment fluorescent microscopic images using Deep Learning methods. The first goal is to implement supervised models to accomplish this task. The second and final goal is to implement an unsupervised model that can perform the same task as well or better than the supervised models. This chapter begins by introducing the dataset that will be used to train, validate, and test these models. The proposed approach for semantic segmentation of cell nuclei and Golgi using an unsupervised model is described in section \ref{section:proposed}. The performance of this approach is compared with the supervised models, which are described in detail in \ref{section:comparison}. Finally, the metrics used to measure the performance of these models are described in section \ref{section:metrics}.

\section{Dataset}
\label{section:dataset}

The dataset used in this work consists of eight crops extracted from \ac{3D} fluorescence microscopy images of mouse retinas. These crops range in size from 257x505x55 to 627x818x61. In these crops, the nuclei are labeled with green fluorescent protein (GFP) and the Golgi are labeled with mCherry. In addition, manually labeled segmentation masks are used for training the proposed supervised methods and evaluating their performance. These masks are RGB, with the red channel containing the segmentation mask of the Golgi and the green channel containing the segmentation mask of the nuclei. The blue channel contains the segmentation mask of the nucleus-Golgi pairs, but only for the task of the 3-class segmentation problem. In this case, the input images and the output masks have 3 channels. For the 2-class segmentation problem, the blue channel is not considered, so the input images and the output masks have only two channels (these masks are converted to RGB for visualization but the blue channel is all zeros).

An example of a 3D crop and its corresponding manually labelled segmentation masks for the 2-class task and 3-class task can be found in \ref{fig:dataset}.

\begin{figure}[!htb]
	\centering
	\subfigure[]{\includegraphics[width=5cm]{Images/img_crop1.jpg}}\hfil
\subfigure[]{\includegraphics[width=5cm]{Images/mask2_crop1.jpg}}\hfil 
\subfigure[]{\includegraphics[width=5cm]{Images/mask3_crop1.jpg}} 
	\caption[(a) Example of a 3D crop of a microscopy image of mouse retina; (b) ground truth mask for 2-class task; (c) ground truth mask for 3-class task.]{(a) Example of a 3D crop of a microscopy image of mouse retina; (b) ground truth mask for 2-class task; (c) ground truth mask for 3-class task.}
	\label{fig:dataset}
\end{figure}

To use these crops to train the models, its necessary to divide them into equal sized patches. If the volumes are too small, not enough information can be learned. If the volumes are too large, training will take longer and could exceed GPU memory. Therefore, we set the size of these patches to 64x64x64. Since the $z$-axis of the original images has a size between 55 and 61, the crops had to be padded to get the 64 slices in the $z$-axis for the input images, for this purpose a reflection padding was applied.

\subsection{Synthetic segmentation masks}
\label{subsection:synthetic_masks}

As mentioned earlier, the manually labelled segmentation masks are used for training the implemented supervised models. However, in order to train the proposed model in a non-supervised way, synthetic segmentation masks had to be created. 

For this purpose, ellipsoids are created to represent the nuclei and spheres for the Golgi at random positions. For each nucleus-Golgi pair created, the radius of the speres (nucleus) and the size of the principal and secondary axis of the ellipsoids (Golgi), the distance between the nucleus and Golgi (relative to the center of the Golgi), and the rotation of each pair are selected randomly from the intervals [5,9] pixels, [[17,30],[8,13],[8,13]] pixels, [-6,6] pixels and [0,180] degrees, respectively. For each crop created, a random number of nucleus-Golgi pairs were selected (between 45 and 70 pairs). To make the images more realistic, an elastic transformation was applied to each nucleus-Golgi pair. Six crops were created with the same size of the six microscopic image crops used for training the model. An example of a synthetically created segmentation mask can be found in figure X (a).

For the 3-class segmentation problem the blue channel 

% \begin{figure}[!htb]
% \centering
% \includegraphics[width=0.50\textwidth]{Images/synthetic.jpg}
% \caption[Example of a synthetic 3D segmentation mask.]{Example of a synthetic 3D segmentation mask.}
% \label{fig:sintetica}
% \end{figure}


\section{Proposed Approach}
\label{section:proposed}

For this work, cell nuclei and Golgi semantic segmentation will be performed using the cycle-consistent GAN (CycleGAN) model. Two different domain images will be considered, the fluorescence microscopy images domain $I$ and the synthetic segmentation mask domain $S$. Moreover, let $i$ and $s$ denote training examples where $i \in I$ and $s \in S$.

Like the original CycleGAN model \cite{cycleGAN:original}, the architecture of this segmentation model will be composed of four interconnected networks, two generators and two discriminators. A representation of this network can be found in Figure \ref{fig:diagrama}. The first generator ($G_S$), corresponding to the segmentation network we wish to obtain, learns a mapping from the domain $I$ to $S$. The first discriminator ($D_S$) takes the segmentation masks as input and tries to predict whether they are real or generated, according to the domain $S$. In contrast, the second generator ($G_I$) learns a mapping from the domain $S$ to $I$, and is used only to improve the training. Furthermore, the second discriminator ($D_I$) receives an image as input and predicts whether this image is real or generated, according to domain $I$.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.99\textwidth]{Images/diagrama.jpg}
  \caption[CycleGAN schematic for the proposed approach.]{CycleGAN schematic for the proposed approach.}
  \label{fig:diagrama}
\end{figure}


\subsection*{Network Architecture}

The generator architecture used in the original CycleGAN model is intended for a 2D image-to-image translation task. Therefore, for our dataset, this approach needs to be adapted to perform a 3D image-to-image translation. This can be achieved by replacing the 2D convolutional layers of the original CycleGAN with a 3D convolutional layers.  The discriminator architecture to be implemented is based on the PatchGAN \cite{isola2018imagetoimage}, which includes LeakyReLU for nonlinearity. The layers of the discriminator architecture must also be converted from 2D to 3D. The detailed architecture of the generator and discriminator used in the CycleGAN model employed in this work are shown in Figures \ref{fig:gencyc} and \ref{fig:disccyc}, respectively.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.99\textwidth]{Images/generator_cyclegan.jpg}
  \caption[Implemented CycleGAN generator architecture.]{Implemented CycleGAN generator architecture.}
  \label{fig:gencyc}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.60\textwidth]{Images/discriminator_cyclegan.jpg}
  \caption[Implemented CycleGAN discriminator architecture.]{Implemented CycleGAN discriminator architecture.}
  \label{fig:disccyc}
\end{figure}

In the generator, the residual blocks consist of an \ac{3D} convolutional layer with instance normalization and ReLU as the activation function, followed by a second convolutional layer also with instance normalization.  The output of this block is the concatenation of the output of the second layer with the input layer of the block.

\subsection*{Loss functions}

The loss function used to train CycleGAN comprises four terms. These terms are described below.

\textbf{Adversarial Loss.} The adversarial term encourages the mapping functions to translate from one domain to
the other. First, the negative log likelihood objective from $\mathcal{L}_{GAN}$  (Equation \ref{eq:GAN}) is replaced by a least-squares loss, since according to \cite{cycleGAN:original}, this loss is more stable during training and produces higher quality results. Therefore, the adversarial loss for the first GAN ($G_{S}, D_S$), for the already mentioned notation, is given by,

\begin{equation}
    \mathcal{L}_{adv}(G_{S},D_S) = \mathbb{E}_{s \sim p_{data}(s)} [(D_S(s)-1)^2] + \mathbb{E}_{i \sim p_{data}(i)} [(D_S(G_{S}(i)))^2].
\end{equation}

The adversarial loss for the second GAN ($G_{I}, D_I$), is written as,

\begin{equation}
    \mathcal{L}_{adv}(G_{I},D_I) = \mathbb{E}_{i \sim p_{data}(i)} [(D_I(i)-1)^2] + \mathbb{E}_{s \sim p_{data}(s)} [(D_I(G_{I}(s)))^2].
\end{equation}

\textbf{Cycle consistency loss.} The cycle consistency loss, $\mathcal{L}_{cyc}$, was described in section \ref{subsection:CycleGAN} with an expression given by (\ref{eq:CCL}). Transferred to our notation, it has the following form,

\begin{equation} 
\mathcal{L}_{cyc} = \mathbb{E}_{i \sim p_{data}(i)} [||G_{I}(G_{S}(i))-i||_1] + \mathbb{E}_{s \sim p_{data}(s)} [||G_{S}(G_{I}(s))-s||_1].
\label{eq:cly_2}
\end{equation}

\textbf{Identity mapping loss.} Lastly, the identity mapping loss, $\mathcal{L}_{id}$ has also been described in \ref{subsection:CycleGAN} with an expression given by (\ref{eq:identity}). Transferred to our notation, it has the following form,

\begin{equation}
    \mathcal{L}_{id} = \mathbb{E}_{i \sim p_{data}(i)} [||G_{I}(i)-i||_1] +  \mathbb{E}_{s \sim p_{data}(s)} [||G_{S}(s)-s||_1].
\end{equation}

Finally, the total loss is obtained by combining by combining the 4 terms:

\begin{equation}
\label{eq:total-cyclegan-loss}
    \mathcal{L}(G_{IS},G_{SI},D_S,D_I) = \mathcal{L}_{adv}(G_{IS},D_S) + \mathcal{L}_{adv}(G_{SI},D_I) + \lambda_{cyc} \mathcal{L}_{cyc} + \lambda_{id} \mathcal{L}_{id}
\end{equation}

\noindent where $\lambda_{cyc}$ and $\lambda_{id}$ are hyperparameters to optimize that control the relative importance of the two objectives. Thus, the objective function is:

\begin{equation}
    \arg \min_{G_{IS},G_{SI}}\max_{D_S,D_I} \mathcal{L}(G_{IS},G_{SI},D_S,D_I).
\end{equation}

\section{Comparison between different approaches}
\label{section:comparison}

The performance of the proposed approach will be compared with two other known supervised techniques for segmenting nuclei and Golgi in fluorescence microscopy images, the 3D U-Net \cite{Unet:3D} and Vox2Vox, which is the 3D extension of the Pix2Pix model \cite{isola2018imagetoimage}. To train these models, the 3D volumes of fluorescence microscopy images and corresponding 3D segmentation masks are required.

The following subsections provide information on the implementation of these methods.

\subsection{3D U-Net}

The 3D U-Net architecture is used to predict two binary segmentation maps, one for the nuclei and other for the Golgi.

The \ac{DC} is a widely used metric in the computer vision community to calculate the similarity between two images \cite{diceloss}. Therefore, the dice coefficient has been fitted to a loss function known as \ac{DLoss}.

\begin{equation}
    DLoss(y,\hat{p}) = 1 - \frac{2y\hat{p}+1}{y+\hat{p}+1}
    \label{eq:dice-loss}
\end{equation}

Here $y$ is the reference value and $\hat{p}$ is the probability value predicted by the model. Here, 1 is added to both the numerator and denominator to ensure that the function is not undefined in edge case scenarios, e.g., when $y=\hat{p}=0$. Unlike the original 3D U-Net model \cite{Unet:3D} previously described in \ref{section:CNN&UNET}, the loss function used in training the model is the dice loss in \ref{eq:dice-loss}.

The implemented architecture, based on the one described in section \ref{section:CNN&UNET} (figure \ref{fig:3dUnet}), is demonstrated in figure \ref{fig:y-unet-3d}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.95\textwidth]{Images/Picture1.jpg}
  \caption[Implemented 3D U-Net architecture.]{Implemented 3D U-Net architecture.}
  \label{fig:y-unet-3d}
\end{figure}

\subsection{Vox2Vox}

Pix2Pix is a supervised GAN model designed for image-to-image translation, previously described in \ref{subsection:pix2pix}. However, it is not capable of performing image-to-image  at the 3D level. A 3D volume-to-volume network for segmentation used as an alternative to Pix2Pix is the Vox2Vox network.

\citet{vox2vox} used a Vox2Vox approach in their work. For this work, it will be used a similar architecture. The generator model is built in the style of the U-Net and Res-Net \cite{2015deep} architectures, while the discriminator is built in the style of the PatchGAN \cite{isola2018imagetoimage}  architecture. The implemented architectures are shown in Figures \ref{fig:genvox} and \ref{fig:discvox}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.95\textwidth]{Images/Picture2.jpg}
  \caption[Implemented Vox2Vox generator architecture.]{Implemented Vox2Vox generator architecture.}
  \label{fig:genvox}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.95\textwidth]{Images/discriminator_vox.jpg}
  \caption[Implemented Vox2Vox discriminator architecture.]{Implemented Vox2Vox discriminator architecture.}
  \label{fig:discvox}
\end{figure}

Another important part of the implementation of Vox2Vox model is the loss function. The discriminator loss, $\mathcal{L}_{disc}$, is the sum of the $L_2$ error of the discriminator output between the original image $x$ and the respective ground-truth $y$ with a tensor of ones, and the $L_2$ error of the discriminator output between the original image and the respective segmentation prediction $\hat{y}$ given by the generator with a tensor of zeros. This can be formulated as follows:

\begin{equation}
    \mathcal{L}_{disc} = L_2[D(x,y),\boldsymbol{1}] + L_2[D(x,\hat{y}),\boldsymbol{0}].
\end{equation}

On the other hand, the generator loss, $\mathcal{L}_{gen}$, is the sum of the $L_2$ error of the discriminator output between the original image, $x$, and the corresponding segmentation prediction, $\hat{y}$, with a tensor of ones, and the \ac{DLoss}, between the ground-truth and the generator output multiplied by the scalar weight coefficient $\alpha > 0$. This can be formulated in the following way:

\begin{equation}
\label{eq:vox2vox-generator-loss}
    \mathcal{L}_{gen} = L_2[D(x,\hat{y}),\boldsymbol{1}] + \alpha DLoss(y,\hat{y}).
\end{equation}

\section{Models Inference}
\label{section:models_inference}

As indicated in \ref{section:dataset}, the input size of the proposed models is 64x64x64. Therefore, to obtain the predicted segmentation mask of an entire crop, it is necessary to slide a 3D window of size 64x64x64 to segment the entire crop. To make the segmentation mask the same size as the original crop, we need to pad the X and Y axes of the original image so that the dimensions are $X_{pad} = (X \% step) \times step $ and $Y_{pad} = (Y \% step) \times step $, where $step$ is the number of pixels the 3D window moves during inference. The padding used is reflection padding. If you place a 3D window in the upper left corner of the padded crop a 64x64x64 patch becomes the input volume of the model used to create a sub-volume of the segmentation mask in the upper left. This 3D window is slid in x and y direction with a certain $step$ through the entire padded crop until the entire volume is processed.


\section{Segmentation performance metrics}
\label{section:metrics}

Pixel-based metrics are used to evaluate the performance of the different models. Specifically, precision, recall, and Dice Coefficient are used to evaluate these different approaches. The equations for these metrics are as follows:

\begin{equation}
    Precision = \frac{n_{TP}}{n_{TP}+n_{FP}},
\end{equation}

\begin{equation}
    Recall = \frac{n_{TP}}{n_{TP}+n_{FN}},
\end{equation}

\begin{equation}
 DICE =\frac{2 \times n_{TP}}{(n_{TP}+n_{FP})+(n_{TP}+n_{FN})}
\end{equation}

\noindent where $n_{TP}$ , $n_{FP}$ and $n_{FN}$ are defined as the number of true-positive, false-positive, and false-negative segmentation result pixels in an image, respectively. These metrics are calculated separately for the objects to be segmented: nuclei, Golgi and nucleus-Golgi pair. A higher \ac{DC} indicates better intersection between the ground truth and the predicted segmentation masks. Low precision may indicate over segmentation and low recall may indicate under segmentation.

\section{Execution time}

The execution time of a deep neural network model can be divided into two intervals: the training time and the testing time. Here, the training time corresponds to the time it takes for a model to learn a particular task. The test time is the time required for a model to make a segmentation prediction for an image. In this work, the execution time is also considered as a measure of the performance of the different approaches. For supervised methods that require manual annotations, the time required to create these masks is also considered.